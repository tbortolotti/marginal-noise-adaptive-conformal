{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bigearth/lib/python3.8/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.0.3) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import hydra\n",
    "import json\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from hydra.utils import instantiate\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pathlib import Path\n",
    "#from torch.utils.data import SequentialSampler, SubsetRandomSampler\n",
    "\n",
    "import torch.nn.functional as F\n",
    "#from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../third_party\")\n",
    "\n",
    "from cln import data\n",
    "from cln import contamination\n",
    "from cln import estimation\n",
    "from cln.utils import evaluate_predictions, estimate_rho\n",
    "from cln.classification import MarginalLabelNoiseConformal\n",
    "\n",
    "from third_party import arc\n",
    "from third_party import bigearthnet\n",
    "\n",
    "from third_party.bigearthnet.datamodules.bigearthnet_datamodule import BigEarthNetDataModule\n",
    "from third_party.bigearthnet.models.bigearthnet_module import BigEarthNetModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define default parameters\n",
    "batch_size = 15\n",
    "epsilon_n_clean = 0.1\n",
    "epsilon_n_corr = 0.1\n",
    "estimate = \"none\"\n",
    "seed = 1\n",
    "\n",
    "# Parse input parameters\n",
    "if False:\n",
    "    print ('Number of arguments:', len(sys.argv), 'arguments.')\n",
    "    print ('Argument List:', str(sys.argv))\n",
    "    if len(sys.argv) != 6:\n",
    "        print(\"Error: incorrect number of parameters.\")\n",
    "        quit()\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    batch_size = int(sys.argv[1])\n",
    "    epsilon_n_clean = float(sys.argv[2])\n",
    "    epsilon_n_corr = float(sys.argv[3])\n",
    "    estimate = sys.argv[4]\n",
    "    seed = int(sys.argv[5])\n",
    "\n",
    "\n",
    "# Define other constant parameters\n",
    "exp_num=201\n",
    "data_name = \"bigearthnet\"\n",
    "#epsilon = 0.017\n",
    "epsilon = 0.03\n",
    "K = 8\n",
    "epsilon_n = epsilon_n_clean + epsilon_n_corr\n",
    "n_test = 500\n",
    "num_exp = 5\n",
    "allow_empty = True\n",
    "epsilon_max = 0.1\n",
    "asymptotic_h_start = 1/400\n",
    "asymptotic_MC_samples = 10000\n",
    "\n",
    "# Parameters of the contamination process\n",
    "contamination_model = \"uniform\"\n",
    "nu = 0.4\n",
    "rho = np.array([0.12, 0.02, 0.11, 0.15, 0.60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process parameters\n",
    "n_cal = batch_size - n_test\n",
    "\n",
    "with open('../third_party/bigearthnet/data/label_mapping.json', 'r') as f:\n",
    "    label_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any previous Hydra instances if they exist\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "hydra.initialize(config_path=\"../third_party/bigearthnet/configs\", version_base=\"1.2\")\n",
    "\n",
    "# fetch the transforms used in the model\n",
    "cfg = hydra.compose(config_name=\"config\")\n",
    "transforms = instantiate(cfg.transforms.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../third_party/datasets/bigearthnet-medium/train loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/val loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/test loaded successfully.\n",
      "\n",
      "Output file: results/exp201/bigearthnet_n15_encl0.1_enco0.1_estnone_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# instantiate the datamodule\n",
    "random_seed = 2\n",
    "datamodule = BigEarthNetDataModule(\n",
    "    cfg.datamodule.dataset_dir,\n",
    "    cfg.datamodule.dataset_name,\n",
    "    #cfg.datamodule.batch_size,\n",
    "    batch_size,\n",
    "    cfg.datamodule.num_workers,\n",
    "    transforms,\n",
    "    label_mapping,\n",
    "    random_seed,\n",
    ")\n",
    "datamodule.setup()\n",
    "\n",
    "# Add important parameters to table of results\n",
    "header = pd.DataFrame({'data':[data_name], 'K':[K],\n",
    "                       'n_cal':[n_cal], 'n_test':[n_test],\n",
    "                       'epsilon_n_clean':[epsilon_n_clean], 'epsilon_n_corr':[epsilon_n_corr],\n",
    "                       'estimate':[estimate], 'seed':[seed]})\n",
    "\n",
    "# Output file\n",
    "outfile_prefix = \"exp\"+str(exp_num) + \"/\" + data_name + \"_n\" + str(batch_size)\n",
    "outfile_prefix += \"_encl\" + str(epsilon_n_clean) + \"_enco\" + str(epsilon_n_corr)\n",
    "outfile_prefix += \"_est\" + estimate + \"_\" + str(seed)\n",
    "print(\"Output file: {:s}.\".format(\"results/\"+outfile_prefix), end=\"\\n\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 5, 5, 5, 5, 1, 3, 5, 3, 5, 5, 5, 3, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get reproducible random samples\n",
    "dataloader_train = datamodule.train_dataloader()\n",
    "dataloader_iter = iter(dataloader_train)\n",
    "batch = next(dataloader_iter)\n",
    "X_batch_train = batch['data']\n",
    "Y_batch_train = batch['labels']\n",
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1v2_corresp = pd.read_csv('../third_party/bigearthnet/data/train_bigearthnet-medium.csv', header=0)\n",
    "#v1v2_corresp['v1-labels-grouped'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(datamodule.random_seed)\n",
    "indices_df = torch.randperm(len(datamodule.train_dataset), generator=generator).tolist()\n",
    "shuffled_csv_df = v1v2_corresp.iloc[indices_df].reset_index(drop=True)\n",
    "batch_size_train = int(batch_size)\n",
    "batch_df_train = shuffled_csv_df.iloc[0 : batch_size_train]\n",
    "Y_labels = batch_df_train['v2-labels-grouped'].to_numpy()\n",
    "valid_indices = torch.tensor(~np.isnan(Y_labels), dtype=torch.bool)\n",
    "X_batch_train = X_batch_train[valid_indices,:,:,:]\n",
    "Y_labels_train = Y_labels[valid_indices].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../third_party/datasets/bigearthnet-medium/train loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/val loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/test loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../third_party/datasets/bigearthnet-medium/train loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/val loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/test loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../third_party/datasets/bigearthnet-medium/train loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/val loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/test loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../third_party/datasets/bigearthnet-medium/train loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/val loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/test loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../third_party/datasets/bigearthnet-medium/train loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/val loaded successfully.\n",
      "\n",
      "../third_party/datasets/bigearthnet-medium/test loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rho: [0.11407945 0.03152507 0.0260395  0.13806038 0.00080216 0.68949345]\n",
      "Mean Rho_tilde: [0.11257252 0.03112403 0.02483612 0.13759417 0.01631555 0.6775576 ]\n",
      "Mean Epsilon: 0.015947036970942064\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20000\n",
    "\n",
    "all_classes = [0, 1, 2, 3, 4, 5]\n",
    "rho_sum = np.zeros(len(all_classes))\n",
    "rho_tilde_sum = np.zeros(len(all_classes))\n",
    "epsilon_sum = 0\n",
    "\n",
    "for i in range(5):\n",
    "    random_seed = int(i)\n",
    "    datamodule = BigEarthNetDataModule(\n",
    "        cfg.datamodule.dataset_dir,\n",
    "        cfg.datamodule.dataset_name,\n",
    "        #cfg.datamodule.batch_size,\n",
    "        batch_size,\n",
    "        cfg.datamodule.num_workers,\n",
    "        transforms,\n",
    "        label_mapping,\n",
    "        random_seed,\n",
    "    )\n",
    "    datamodule.setup()\n",
    "\n",
    "    # Get reproducible random samples\n",
    "    dataloader_train = datamodule.train_dataloader()\n",
    "    dataloader_iter = iter(dataloader_train)\n",
    "    batch = next(dataloader_iter)\n",
    "    X_batch_train = batch['data']\n",
    "    Yt_batch_train = batch['labels']\n",
    "    batch['labels']\n",
    "\n",
    "    v1v2_corresp = pd.read_csv('../third_party/bigearthnet/data/train_bigearthnet-medium.csv', header=0)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(datamodule.random_seed)\n",
    "    indices_df = torch.randperm(len(datamodule.train_dataset), generator=generator).tolist()\n",
    "    shuffled_csv_df = v1v2_corresp.iloc[indices_df].reset_index(drop=True)\n",
    "    batch_size_train = int(batch_size)\n",
    "    batch_df_train = shuffled_csv_df.iloc[0 : batch_size_train]\n",
    "    Y_batch_train = batch_df_train['v2-labels-grouped'].to_numpy()\n",
    "\n",
    "    valid_indices = torch.tensor(~np.isnan(Y_batch_train), dtype=torch.bool)\n",
    "\n",
    "    X_batch_train = X_batch_train[valid_indices,:,:,:]\n",
    "    Yt_batch_train = Yt_batch_train[valid_indices].detach().numpy().astype(int)\n",
    "    Y_batch_train = Y_batch_train[valid_indices].astype(int)\n",
    "\n",
    "    # Compute rho and rho_tilde for the current batch\n",
    "    rho_current = np.zeros(len(all_classes))\n",
    "    rho_tilde_current = np.zeros(len(all_classes))\n",
    "\n",
    "    unique, counts = np.unique(Y_batch_train, return_counts=True)\n",
    "    for value, count in zip(unique, counts):\n",
    "        rho_current[value] = count / len(Y_batch_train)\n",
    "\n",
    "    unique, counts = np.unique(Yt_batch_train, return_counts=True)\n",
    "    for value, count in zip(unique, counts):\n",
    "        rho_tilde_current[value] = count / len(Yt_batch_train)\n",
    "\n",
    "    # Accumulate values\n",
    "    rho_sum += rho_current\n",
    "    rho_tilde_sum += rho_tilde_current\n",
    "\n",
    "    # Compute epsilon\n",
    "    epsilon = np.sum(Yt_batch_train != Y_batch_train) / len(Y_batch_train)\n",
    "    epsilon_sum += epsilon\n",
    "\n",
    "    #print(epsilon)\n",
    "    #print(Yt_batch_train)\n",
    "    #print(Y_batch_train)\n",
    "\n",
    "rho_mean = rho_sum / 5\n",
    "rho_tilde_mean = rho_tilde_sum / 5\n",
    "epsilon_mean = epsilon_sum / 5\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean Rho: {rho_mean}\")\n",
    "print(f\"Mean Rho_tilde: {rho_tilde_mean}\")\n",
    "print(f\"Mean Epsilon: {epsilon_mean}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.115, 0.032, 0.025, 0.137, 0.001, 0.69]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho = [0.115, 0.032, 0.025, 0.137, 0.001, 0.690]\n",
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize noise contamination process\n",
    "if contamination_model == \"uniform\":\n",
    "    T = contamination.construct_T_matrix_simple(K, epsilon)\n",
    "    M = contamination.convert_T_to_M(T,rho)\n",
    "elif contamination_model == \"block\":\n",
    "    T = contamination.construct_T_matrix_block(K, epsilon)\n",
    "    M = contamination.convert_T_to_M(T,rho)\n",
    "elif contamination_model == \"RRB\":\n",
    "    T = contamination.construct_T_matrix_block_RR(K, epsilon, nu)\n",
    "    M = contamination.convert_T_to_M(T,rho)\n",
    "elif contamination_model == \"random\":\n",
    "    T = contamination.construct_T_matrix_random(K, epsilon, random_state=seed)\n",
    "    M = contamination.convert_T_to_M(T,rho)\n",
    "else:\n",
    "    print(\"Unknown contamination (M) model!\")\n",
    "    sys.stdout.flush()\n",
    "    exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "black_box = BigEarthNetModule(cfg)\n",
    "mod_dir = cfg.out_directory.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_name = \"trained_model.pth\"\n",
    "mod_path = os.path.join(mod_dir, mod_name)\n",
    "black_box.load_state_dict(torch.load(mod_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the experiment\n",
    "def run_experiment(random_state):\n",
    "    print(\"\\nRunning experiment in batch {:d}...\".format(random_state))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Generate a large data set\n",
    "    print(\"\\nGenerating data...\", end=' ')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Get reproducible random samples\n",
    "    dataloader_train = datamodule.train_dataloader()\n",
    "    dataloader_val = datamodule.val_dataloader()\n",
    "    dataloader_test = datamodule.test_dataloader()\n",
    "\n",
    "    batch = next(iter(dataloader_train))\n",
    "    X_batch_train = batch['data']\n",
    "    Y_batch_train = batch['labels']\n",
    "\n",
    "    batch = next(iter(dataloader_val))\n",
    "    X_batch_val = batch['data']\n",
    "    Y_batch_val = batch['labels']\n",
    "\n",
    "    batch = next(iter(dataloader_test))\n",
    "    X_batch_test = batch['data']\n",
    "    Y_batch_test = batch['labels']\n",
    "\n",
    "    # Stack all features and labels together\n",
    "    X_batch = torch.cat((X_batch_train, X_batch_val, X_batch_test), dim=0)\n",
    "    Y_batch = torch.cat((Y_batch_train, Y_batch_val, Y_batch_test), dim=0)\n",
    "    Y_batch = Y_batch.detach().numpy()\n",
    "    print(\"Done.\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    ## TO DO ##\n",
    "    #' Load Y_batch from file.... deve essere un numpy array\n",
    "    # Y_batch = load from file i clean labels, oppure aggiungi una qualche corruption di qualche tipo\n",
    "    #Y_batch = Yt_batch\n",
    "\n",
    "    # Generate the contaminated labels\n",
    "    print(\"Generating contaminated labels...\", end=' ')\n",
    "    sys.stdout.flush()\n",
    "    contamination_process = contamination.LinearContaminationModel(T, random_state=random_state+3)\n",
    "    Yt_batch = contamination_process.sample_labels(Y_batch)\n",
    "    print(\"Done.\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Estimate the label proportions from the whole data set\n",
    "    rho = estimate_rho(Y_batch, K)\n",
    "    rho_tilde = estimate_rho(Yt_batch, K)\n",
    "\n",
    "    # Separate the test set\n",
    "    X, X_test, Y, Y_test, Yt, _ = train_test_split(X_batch, Y_batch, Yt_batch, test_size=n_test, random_state=random_state+2)\n",
    "\n",
    "    # Estimate (if applicable) the label contamination model\n",
    "    if estimate==\"none\":\n",
    "        rho_hat = rho\n",
    "        rho_tilde_hat = rho_tilde\n",
    "        T_hat = contamination.construct_T_matrix_simple(K, epsilon)\n",
    "        M_hat = contamination.convert_T_to_M(T_hat, rho_hat)\n",
    "        epsilon_ci = None\n",
    "        epsilon_hat = np.nan\n",
    "    elif estimate==\"rho\":\n",
    "        rho_tilde_hat = estimate_rho(Yt, K)\n",
    "        T_hat = contamination.construct_T_matrix_simple(K, epsilon)  \n",
    "        M_hat = contamination.convert_T_to_M(T_hat,rho)\n",
    "        rho_hat = np.dot(M_hat.T, rho_tilde_hat)\n",
    "        epsilon_ci = None\n",
    "        epsilon_hat = np.nan\n",
    "    elif estimate==\"rho-epsilon-point\":\n",
    "        # Hold-out some data to estimate the contamination model\n",
    "        X, X_estim, Y, Y_estim, Yt, Yt_estim = train_test_split(X, Y, Yt, test_size=epsilon_n, random_state=random_state+3)\n",
    "\n",
    "        # Keep some hold-out data clean\n",
    "        X_estim_clean, X_estim_corr, Y_estim_clean, _, _, Yt_estim_corr = train_test_split(X_estim, Y_estim, Yt_estim,\n",
    "                                                                                           test_size=epsilon_n_corr/epsilon_n, random_state=random_state+4)\n",
    "\n",
    "        rho_tilde_hat = estimate_rho(Yt, K)\n",
    "        epsilon_hat, _, _, _, _ = estimation.fit_contamination_model_RR(X_estim_clean, X_estim_corr,\n",
    "                                                                        Y_estim_clean, Yt_estim_corr, black_box,\n",
    "                                                                        K, 0.01, pre_trained=True,\n",
    "                                                                        random_state=random_state+6)\n",
    "        T_hat = contamination.construct_T_matrix_simple(K, epsilon_hat)\n",
    "        rho_hat = np.dot(np.linalg.inv(T_hat), rho_tilde_hat)\n",
    "        M_hat = contamination.convert_T_to_M(T_hat,rho_hat)\n",
    "        epsilon_ci = None\n",
    "\n",
    "    else:\n",
    "        print(\"Unknown estimation option!\")\n",
    "        sys.stdout.flush()\n",
    "        exit(-1)\n",
    "\n",
    "\n",
    "    res = pd.DataFrame({})\n",
    "    for alpha in [0.1]:\n",
    "        for guarantee in ['marginal']:\n",
    "\n",
    "            print(\"\\nSeeking {:s} coverage at level {:.2f}.\".format(guarantee, 1-alpha))\n",
    "\n",
    "            label_conditional = False\n",
    "            alpha_theory = alpha * (1 - epsilon * (1-1/K))\n",
    "\n",
    "            # Define a dictionary of methods with their names and corresponding initialization parameters\n",
    "            methods = {\n",
    "                \"Standard\": lambda: arc.methods.SplitConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                               label_conditional=label_conditional, allow_empty=allow_empty,\n",
    "                                                               pre_trained=True, random_state=random_state),\n",
    "                \n",
    "                \"Standard (theory)\": lambda: arc.methods.SplitConformal(X, Yt, black_box, K, alpha_theory, n_cal=-1,\n",
    "                                                               label_conditional=label_conditional, allow_empty=allow_empty,\n",
    "                                                               pre_trained=True, random_state=random_state),\n",
    "\n",
    "                \"Adaptive\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                epsilon=epsilon, T=T_hat, M=M_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                allow_empty=allow_empty, method=\"old\", optimistic=False,\n",
    "                                                                verbose=False, pre_trained=True, random_state=random_state),\n",
    "\n",
    "                \"Adaptive+\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                epsilon=epsilon, T=T_hat, M=M_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                allow_empty=allow_empty, method=\"old\", optimistic=True,\n",
    "                                                                verbose=False, pre_trained=True, random_state=random_state),\n",
    "\n",
    "                \"Adaptive optimized\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                          epsilon=epsilon, T=T_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                          allow_empty=allow_empty, method=\"improved\",\n",
    "                                                                          optimized=True, optimistic=False, verbose=False,\n",
    "                                                                          pre_trained=True, random_state=random_state),\n",
    "\n",
    "                \"Adaptive optimized+\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                          epsilon=epsilon, T=T_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                          allow_empty=allow_empty, method=\"improved\",\n",
    "                                                                          optimized=True, optimistic=True, verbose=False,\n",
    "                                                                          pre_trained=True, random_state=random_state),\n",
    "\n",
    "                \"Adaptive simplified\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                           epsilon=epsilon, T=T_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                           allow_empty=allow_empty, method=\"improved\",\n",
    "                                                                           optimized=False, optimistic=False, verbose=False,\n",
    "                                                                           pre_trained=True, random_state=random_state),\n",
    "\n",
    "                \"Adaptive simplified+\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                           epsilon=epsilon, T=T_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                           allow_empty=allow_empty, method=\"improved\",\n",
    "                                                                           optimized=False, optimistic=True, verbose=False,\n",
    "                                                                           pre_trained=True, random_state=random_state),\n",
    "\n",
    "                \"Asymptotic\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                  epsilon=epsilon, asymptotic_h_start=asymptotic_h_start,\n",
    "                                                                  asymptotic_MC_samples=asymptotic_MC_samples, T=T_hat,\n",
    "                                                                  rho_tilde=rho_tilde_hat, allow_empty=allow_empty,\n",
    "                                                                  method=\"asymptotic\", optimistic=False, verbose=False,\n",
    "                                                                  pre_trained=True, random_state=random_state),\n",
    "\n",
    "                \"Asymptotic+\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                   epsilon=epsilon, asymptotic_h_start=asymptotic_h_start,\n",
    "                                                                   asymptotic_MC_samples=asymptotic_MC_samples, T=T_hat,\n",
    "                                                                   rho_tilde=rho_tilde_hat, allow_empty=allow_empty,\n",
    "                                                                   method=\"asymptotic\", optimistic=True, verbose=False,\n",
    "                                                                   pre_trained=True, random_state=random_state)\n",
    "\n",
    "            }\n",
    "\n",
    "            # Initialize an empty list to store the evaluation results\n",
    "            res_list = []\n",
    "\n",
    "            # Loop through the methods, apply them, and evaluate the results\n",
    "            for method_name, method_func in methods.items():\n",
    "                print(f\"Applying {method_name} method...\", end=' ')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                # Initialize and apply the method\n",
    "                method = method_func()\n",
    "                predictions = method.predict(X_test)\n",
    "\n",
    "                print(\"Done.\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                # Evaluate the method\n",
    "                res_new = evaluate_predictions(predictions, X_test, Y_test, K, verbose=False)\n",
    "                res_new['Method'] = method_name\n",
    "                res_new['Guarantee'] = guarantee\n",
    "                res_new['Alpha'] = alpha\n",
    "                res_new['random_state'] = random_state\n",
    "\n",
    "                # Append the result to the results list\n",
    "                res_list.append(res_new)\n",
    "\n",
    "\n",
    "            # Combine all results into a single DataFrame\n",
    "            res = pd.concat(res_list)\n",
    "                \n",
    "    print(res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments\n",
    "results = pd.DataFrame({})\n",
    "for batch in np.arange(1,2):\n",
    "    res = run_experiment(1000*seed+batch-1000)\n",
    "    results = pd.concat([results, res])\n",
    "\n",
    "    # Save results\n",
    "    outfile = \"results/\" + outfile_prefix + \".txt\"\n",
    "    results_out = pd.concat([header,results], axis=1)\n",
    "    results_out.to_csv(outfile, index=False, float_format=\"%.5f\")\n",
    "\n",
    "print(\"\\nPreview of results:\")\n",
    "print(results)\n",
    "sys.stdout.flush()\n",
    "\n",
    "print(\"\\nSummary of results:\")\n",
    "summary = results.groupby(['Alpha', 'Guarantee', 'Method', 'Label']).agg(['mean','std']).reset_index()\n",
    "print(summary)\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "print(\"\\nFinished.\\nResults written to {:s}\\n\".format(outfile))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=1\n",
    "\n",
    "print(\"\\nRunning experiment in batch {:d}...\".format(random_state))\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Generate a large data set\n",
    "print(\"\\nGenerating data...\", end=' ')\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Get reproducible random samples\n",
    "dataloader_train = datamodule.train_dataloader()\n",
    "dataloader_val = datamodule.val_dataloader()\n",
    "dataloader_test = datamodule.test_dataloader()\n",
    "\n",
    "batch = next(iter(dataloader_train))\n",
    "X_batch_train = batch['data']\n",
    "Y_batch_train = batch['labels']\n",
    "\n",
    "batch = next(iter(dataloader_val))\n",
    "X_batch_val = batch['data']\n",
    "Y_batch_val = batch['labels']\n",
    "\n",
    "batch = next(iter(dataloader_test))\n",
    "X_batch_test = batch['data']\n",
    "Y_batch_test = batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all features and labels together\n",
    "X_batch = torch.cat((X_batch_train, X_batch_val, X_batch_test), dim=0)\n",
    "Y_batch = torch.cat((Y_batch_train, Y_batch_val, Y_batch_test), dim=0)\n",
    "Y_batch = Y_batch.detach().numpy()\n",
    "print(\"Done.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts = np.unique(Y_batch, return_counts=True)\n",
    "\n",
    "# Display the results\n",
    "frequencies = dict(zip(unique_elements, counts))\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the contaminated labels\n",
    "print(\"Generating contaminated labels...\", end=' ')\n",
    "sys.stdout.flush()\n",
    "contamination_process = contamination.LinearContaminationModel(T, random_state=random_state+3)\n",
    "Yt_batch = contamination_process.sample_labels(Y_batch)\n",
    "print(\"Done.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_batch[:10])\n",
    "print(Yt_batch[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the label proportions from the whole data set\n",
    "rho = estimate_rho(Y_batch, K)\n",
    "rho_tilde = estimate_rho(Yt_batch, K)\n",
    "\n",
    "# Separate the test set\n",
    "X, X_test, Y, Y_test, Yt, _ = train_test_split(X_batch, Y_batch, Yt_batch, test_size=n_test, random_state=random_state+2)\n",
    "\n",
    "# Estimate (if applicable) the label contamination model\n",
    "if estimate==\"none\":\n",
    "    rho_hat = rho\n",
    "    rho_tilde_hat = rho_tilde\n",
    "    T_hat = contamination.construct_T_matrix_simple(K, epsilon)\n",
    "    M_hat = contamination.convert_T_to_M(T_hat, rho_hat)\n",
    "    epsilon_ci = None\n",
    "    epsilon_hat = np.nan\n",
    "elif estimate==\"rho\":\n",
    "    rho_tilde_hat = estimate_rho(Yt, K)\n",
    "    T_hat = contamination.construct_T_matrix_simple(K, epsilon)  \n",
    "    M_hat = contamination.convert_T_to_M(T_hat,rho)\n",
    "    rho_hat = np.dot(M_hat.T, rho_tilde_hat)\n",
    "    epsilon_ci = None\n",
    "    epsilon_hat = np.nan\n",
    "elif estimate==\"rho-epsilon-point\":\n",
    "    # Hold-out some data to estimate the contamination model\n",
    "    X, X_estim, Y, Y_estim, Yt, Yt_estim = train_test_split(X, Y, Yt, test_size=epsilon_n, random_state=random_state+3)\n",
    "\n",
    "    # Keep some hold-out data clean\n",
    "    X_estim_clean, X_estim_corr, Y_estim_clean, _, _, Yt_estim_corr = train_test_split(X_estim, Y_estim, Yt_estim,\n",
    "                                                                                       test_size=epsilon_n_corr/epsilon_n, random_state=random_state+4)\n",
    "\n",
    "    rho_tilde_hat = estimate_rho(Yt, K)\n",
    "    epsilon_hat, _, _, _, _ = estimation.fit_contamination_model_RR(X_estim_clean, X_estim_corr,\n",
    "                                                                        Y_estim_clean, Yt_estim_corr, black_box,\n",
    "                                                                        K, 0.01, pre_trained=True,\n",
    "                                                                        random_state=random_state+6)\n",
    "    T_hat = contamination.construct_T_matrix_simple(K, epsilon_hat)\n",
    "    rho_hat = np.dot(np.linalg.inv(T_hat), rho_tilde_hat)\n",
    "    M_hat = contamination.convert_T_to_M(T_hat,rho_hat)\n",
    "    epsilon_ci = None\n",
    "\n",
    "else:\n",
    "    print(\"Unknown estimation option!\")\n",
    "    sys.stdout.flush()\n",
    "    exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({})\n",
    "for alpha in [0.1]:\n",
    "    for guarantee in ['marginal']:\n",
    "\n",
    "        print(\"\\nSeeking {:s} coverage at level {:.2f}.\".format(guarantee, 1-alpha))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        label_conditional = False\n",
    "        alpha_theory = alpha * (1 - epsilon * (1-1/K))\n",
    "\n",
    "        # Define a dictionary of methods with their names and corresponding initialization parameters\n",
    "        methods = {\n",
    "            \"\"\"\n",
    "            \"Standard\": lambda: arc.methods.SplitConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                               label_conditional=label_conditional, allow_empty=allow_empty,\n",
    "                                                               pre_trained=True, random_state=random_state),\n",
    "                \n",
    "            \"Standard (theory)\": lambda: arc.methods.SplitConformal(X, Yt, black_box, K, alpha_theory, n_cal=-1,\n",
    "                                                               label_conditional=label_conditional, allow_empty=allow_empty,\n",
    "                                                               pre_trained=True, random_state=random_state),\n",
    "            \"\"\"\n",
    "\n",
    "            \"Adaptive\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                epsilon=epsilon, T=T_hat, M=M_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                allow_empty=allow_empty, method=\"old\", optimistic=False,\n",
    "                                                                verbose=False, pre_trained=True, random_state=random_state),\n",
    "\n",
    "            \"Adaptive+\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                epsilon=epsilon, T=T_hat, M=M_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                allow_empty=allow_empty, method=\"old\", optimistic=True,\n",
    "                                                                verbose=False, pre_trained=True, random_state=random_state),\n",
    "\n",
    "            \"Adaptive optimized\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                          epsilon=epsilon, T=T_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                          allow_empty=allow_empty, method=\"improved\",\n",
    "                                                                          optimized=True, optimistic=False, verbose=False,\n",
    "                                                                          pre_trained=True, random_state=random_state),\n",
    "\n",
    "            \"Adaptive optimized+\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                          epsilon=epsilon, T=T_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                          allow_empty=allow_empty, method=\"improved\",\n",
    "                                                                          optimized=True, optimistic=True, verbose=False,\n",
    "                                                                          pre_trained=True, random_state=random_state),\n",
    "\n",
    "            \"Adaptive simplified\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                           epsilon=epsilon, T=T_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                           allow_empty=allow_empty, method=\"improved\",\n",
    "                                                                           optimized=False, optimistic=False, verbose=False,\n",
    "                                                                           pre_trained=True, random_state=random_state),\n",
    "\n",
    "            \"Adaptive simplified+\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                           epsilon=epsilon, T=T_hat, rho_tilde=rho_tilde_hat,\n",
    "                                                                           allow_empty=allow_empty, method=\"improved\",\n",
    "                                                                           optimized=False, optimistic=True, verbose=False,\n",
    "                                                                           pre_trained=True, random_state=random_state),\n",
    "\n",
    "            \"Asymptotic\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                  epsilon=epsilon, asymptotic_h_start=asymptotic_h_start,\n",
    "                                                                  asymptotic_MC_samples=asymptotic_MC_samples, T=T_hat,\n",
    "                                                                  rho_tilde=rho_tilde_hat, allow_empty=allow_empty,\n",
    "                                                                  method=\"asymptotic\", optimistic=False, verbose=False,\n",
    "                                                                  pre_trained=True, random_state=random_state),\n",
    "\n",
    "            \"Asymptotic+\": lambda: MarginalLabelNoiseConformal(X, Yt, black_box, K, alpha, n_cal=-1,\n",
    "                                                                   epsilon=epsilon, asymptotic_h_start=asymptotic_h_start,\n",
    "                                                                   asymptotic_MC_samples=asymptotic_MC_samples, T=T_hat,\n",
    "                                                                   rho_tilde=rho_tilde_hat, allow_empty=allow_empty,\n",
    "                                                                   method=\"asymptotic\", optimistic=True, verbose=False,\n",
    "                                                                   pre_trained=True, random_state=random_state)\n",
    "\n",
    "        }\n",
    "    print(\"Done.\")\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "guarantee='marginal'\n",
    "\n",
    "label_conditional = False\n",
    "alpha_theory = alpha * (1 - epsilon * (1-1/K))\n",
    "\n",
    "print(\"\\nSeeking {:s} coverage at level {:.2f}.\".format(guarantee, 1-alpha))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the evaluation results\n",
    "res_list = []\n",
    "\n",
    "# Loop through the methods, apply them, and evaluate the results\n",
    "for method_name, method_func in methods.items():\n",
    "    print(f\"Applying {method_name} method...\", end=' ')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Initialize and apply the method\n",
    "    method = method_func()\n",
    "    predictions = method.predict(X_test)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Evaluate the method\n",
    "    print(f\"Evaluating {method_name} method...\", end=' ')\n",
    "    sys.stdout.flush()\n",
    "    res_new = evaluate_predictions(predictions, X_test, Y_test, K, verbose=False)\n",
    "    res_new['Method'] = method_name\n",
    "    res_new['Guarantee'] = guarantee\n",
    "    res_new['Alpha'] = alpha\n",
    "    res_new['random_state'] = random_state\n",
    "\n",
    "    print(\"Done.\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Append the result to the results list\n",
    "    #res_list.append(res_new)\n",
    "\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    #res = pd.concat(res_list)\n",
    "                \n",
    "#print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the methods, apply them, and evaluate the results\n",
    "\n",
    "for method_name, method_func in methods.items():\n",
    "    print(f\"Applying {method_name} method...\", end=' ')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Initialize and apply the method\n",
    "    method = method_func()\n",
    "    predictions = method.predict(X_test)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Evaluate the method\n",
    "    print(f\"Evaluating {method_name} method...\", end=' ')\n",
    "    sys.stdout.flush()\n",
    "    res_new = evaluate_predictions(predictions, X_test, Y_test, K, verbose=False)\n",
    "    res_new['Method'] = method_name\n",
    "    res_new['Guarantee'] = guarantee\n",
    "    res_new['Alpha'] = alpha\n",
    "    res_new['random_state'] = random_state\n",
    "\n",
    "    print(\"Done.\")\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigearth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
